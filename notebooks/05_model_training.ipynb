{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9153e676-5b0c-48f4-9642-de1b972694a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ XO Project - Model Training & Evaluation Phase\n",
      "=================================================================\n",
      "Objective: Train robust ML models for exoplanet habitability prediction\n",
      "=================================================================\n",
      "Loaded ML-optimized dataset: 1,729 planets, 22 features\n",
      "\n",
      "Dataset overview:\n",
      "Shape: (1729, 22)\n",
      "Target variable: 'ml_target'\n",
      "\n",
      "Class distribution:\n",
      "Not Habitable (0): 1,319 planets (76.3%)\n",
      "Habitable (1):     410 planets (23.7%)\n",
      "Class ratio: 3.2:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1 - Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ü§ñ XO Project - Model Training & Evaluation Phase\")\n",
    "print(\"=\"*65)\n",
    "print(\"Objective: Train robust ML models for exoplanet habitability prediction\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Load optimized dataset from Phase 4\n",
    "df_ml = pd.read_csv('../data/processed/ml_optimized_dataset.csv')\n",
    "print(f\"Loaded ML-optimized dataset: {len(df_ml):,} planets, {len(df_ml.columns)} features\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset overview:\")\n",
    "print(f\"Shape: {df_ml.shape}\")\n",
    "print(f\"Target variable: 'ml_target'\")\n",
    "\n",
    "# Check class distribution\n",
    "target_counts = df_ml['ml_target'].value_counts()\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Not Habitable (0): {target_counts[0]:,} planets ({target_counts[0]/len(df_ml)*100:.1f}%)\")\n",
    "print(f\"Habitable (1):     {target_counts[1]:,} planets ({target_counts[1]/len(df_ml)*100:.1f}%)\")\n",
    "print(f\"Class ratio: {target_counts[0]/target_counts[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eed9b61-2fbb-4e70-b77d-6704c9b253e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Feature Engineering and Data Preparation\n",
      "==================================================\n",
      "Available features: 16\n",
      "\n",
      "Missing value analysis:\n",
      "Features with missing values:\n",
      "  pl_bmasse            | 1,427 missing ( 82.5%)\n",
      "  pl_eqt               | 1,361 missing ( 78.7%)\n",
      "  esi_mass             | 1,427 missing ( 82.5%)\n",
      "  esi_temperature      | 1,361 missing ( 78.7%)\n",
      "  esi_surface          | 1,361 missing ( 78.7%)\n",
      "  escape_velocity_ratio | 1,427 missing ( 82.5%)\n",
      "  stellar_flux         |  265 missing ( 15.3%)\n",
      "\n",
      "Feature matrix shape: (1729, 16)\n",
      "Target vector shape: (1729,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Feature Selection and Data Preparation\n",
    "print(\"\\nüîß Feature Engineering and Data Preparation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define feature columns (excluding target and metadata)\n",
    "feature_columns = [\n",
    "    'pl_rade', 'pl_bmasse', 'pl_orbsmax', 'st_teff', 'st_mass', 'pl_eqt',\n",
    "    'stellar_luminosity', 'hz_position', 'in_habitable_zone',\n",
    "    'esi_radius', 'esi_mass', 'esi_temperature', 'esi_surface',\n",
    "    'escape_velocity_ratio', 'stellar_flux', 'habitability_score'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "available_features = [col for col in feature_columns if col in df_ml.columns]\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "# Analyze missing values\n",
    "print(f\"\\nMissing value analysis:\")\n",
    "missing_summary = df_ml[available_features].isnull().sum()\n",
    "missing_features = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(\"Features with missing values:\")\n",
    "    for feature, count in missing_features.items():\n",
    "        percentage = (count / len(df_ml)) * 100\n",
    "        print(f\"  {feature:20} | {count:4,} missing ({percentage:5.1f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected\")\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = df_ml[available_features].copy()\n",
    "y = df_ml['ml_target'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8600b53d-edee-4745-a4a1-cb7a558c8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Missing Value Imputation\n",
      "========================================\n",
      "‚úÖ Imputed 1,427 values in pl_bmasse using median\n",
      "‚úÖ Imputed 1,361 values in pl_eqt using median\n",
      "‚úÖ Imputed 1,427 values in esi_mass using median\n",
      "‚úÖ Imputed 1,361 values in esi_temperature using median\n",
      "‚úÖ Imputed 1,361 values in esi_surface using median\n",
      "‚úÖ Imputed 1,427 values in escape_velocity_ratio using median\n",
      "‚úÖ Imputed 265 values in stellar_flux using median\n",
      "\n",
      "Remaining missing values: 0\n",
      "‚úÖ All missing values successfully imputed\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 - Missing Value Imputation Strategy\n",
    "print(\"\\nüî¨ Missing Value Imputation\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Strategy: Different imputation for different feature types\n",
    "def impute_features(X):\n",
    "    \"\"\"Smart imputation based on feature characteristics\"\"\"\n",
    "    X_imputed = X.copy()\n",
    "    \n",
    "    # Physics-based imputation\n",
    "    imputation_strategy = {\n",
    "        # Use median for astronomical measurements (robust to outliers)\n",
    "        'pl_bmasse': 'median',\n",
    "        'pl_eqt': 'median', \n",
    "        'esi_mass': 'median',\n",
    "        'esi_temperature': 'median',\n",
    "        'esi_surface': 'median',\n",
    "        'escape_velocity_ratio': 'median',\n",
    "        'stellar_flux': 'median'\n",
    "    }\n",
    "    \n",
    "    for feature, strategy in imputation_strategy.items():\n",
    "        if feature in X_imputed.columns:\n",
    "            if X_imputed[feature].isnull().sum() > 0:\n",
    "                imputer = SimpleImputer(strategy=strategy)\n",
    "                X_imputed[feature] = imputer.fit_transform(X_imputed[[feature]]).ravel()\n",
    "                missing_count = X[feature].isnull().sum()\n",
    "                print(f\"‚úÖ Imputed {missing_count:,} values in {feature} using {strategy}\")\n",
    "    \n",
    "    return X_imputed\n",
    "\n",
    "# Apply imputation\n",
    "X_imputed = impute_features(X)\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = X_imputed.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values: {remaining_missing}\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\"‚úÖ All missing values successfully imputed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some missing values remain - check imputation strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5070e211-0e7d-4115-982c-36e2fb7e1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Train-Test Split and Feature Scaling\n",
      "=============================================\n",
      "Training set: 1,383 samples\n",
      "Test set:     346 samples\n",
      "\n",
      "Training set distribution:\n",
      "  Not Habitable: 1,055 (76.3%)\n",
      "  Habitable:     328 (23.7%)\n",
      "\n",
      "Test set distribution:\n",
      "  Not Habitable: 264 (76.3%)\n",
      "  Habitable:     82 (23.7%)\n",
      "\n",
      "‚úÖ Features scaled using RobustScaler\n",
      "   - Training features: (1383, 16)\n",
      "   - Test features: (346, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Train-Test Split and Scaling\n",
    "print(\"\\nüéØ Train-Test Split and Feature Scaling\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set:     {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "train_dist = y_train.value_counts()\n",
    "test_dist = y_test.value_counts()\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"  Not Habitable: {train_dist[0]:,} ({train_dist[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Habitable:     {train_dist[1]:,} ({train_dist[1]/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set distribution:\")\n",
    "print(f\"  Not Habitable: {test_dist[0]:,} ({test_dist[0]/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Habitable:     {test_dist[1]:,} ({test_dist[1]/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling (important for SVM, Neural Networks, Logistic Regression)\n",
    "scaler = RobustScaler()  # Robust to outliers (better for astronomical data)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Features scaled using RobustScaler\")\n",
    "print(f\"   - Training features: {X_train_scaled.shape}\")\n",
    "print(f\"   - Test features: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ab222b-4d82-4801-a150-f76505c00f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Baseline Model Training\n",
      "===================================\n",
      "Training baseline models with 5-fold cross-validation...\n",
      "\n",
      "üî• Training Logistic Regression...\n",
      "   F1 Score: 0.777\n",
      "   ROC-AUC: 0.950\n",
      "   CV F1: 0.851 ¬± 0.030\n",
      "\n",
      "üî• Training Random Forest...\n",
      "   F1 Score: 0.975\n",
      "   ROC-AUC: 0.998\n",
      "   CV F1: 0.963 ¬± 0.016\n",
      "\n",
      "üî• Training Gradient Boosting...\n",
      "   F1 Score: 0.969\n",
      "   ROC-AUC: 0.998\n",
      "   CV F1: 0.968 ¬± 0.013\n",
      "\n",
      "üî• Training SVM...\n",
      "   F1 Score: 0.422\n",
      "   ROC-AUC: 0.770\n",
      "   CV F1: 0.409 ¬± 0.012\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Baseline Models Training\n",
    "print(\"\\nüéØ Baseline Model Training\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Define baseline models with appropriate parameters for imbalanced data\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42, \n",
    "        class_weight='balanced',\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        learning_rate=0.1\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        kernel='rbf'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate baseline models\n",
    "baseline_results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training baseline models with 5-fold cross-validation...\")\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nüî• Training {name}...\")\n",
    "    \n",
    "    # Determine if model needs scaled features\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_f1_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='f1')\n",
    "    cv_roc_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='roc_auc')\n",
    "    cv_precision_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='precision')\n",
    "    cv_recall_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='recall')\n",
    "    \n",
    "    # Store CV results\n",
    "    cv_scores[name] = {\n",
    "        'F1': cv_f1_scores,\n",
    "        'ROC-AUC': cv_roc_scores,\n",
    "        'Precision': cv_precision_scores,\n",
    "        'Recall': cv_recall_scores\n",
    "    }\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'avg_precision': avg_precision,\n",
    "        'cv_f1_mean': cv_f1_scores.mean(),\n",
    "        'cv_f1_std': cv_f1_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"   F1 Score: {f1:.3f}\")\n",
    "    print(f\"   ROC-AUC: {roc_auc:.3f}\")\n",
    "    print(f\"   CV F1: {cv_f1_scores.mean():.3f} ¬± {cv_f1_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d7339b-160c-44d5-b4c5-522a4e3a3c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Baseline Model Results Summary\n",
      "========================================\n",
      "Baseline Model Performance:\n",
      "                 Model  F1_Score  ROC_AUC  Avg_Precision  CV_F1_Mean  \\\n",
      "1        Random Forest     0.975    0.998          0.994       0.963   \n",
      "2    Gradient Boosting     0.969    0.998          0.995       0.968   \n",
      "0  Logistic Regression     0.777    0.950          0.795       0.851   \n",
      "3                  SVM     0.422    0.770          0.471       0.409   \n",
      "\n",
      "   CV_F1_Std  \n",
      "1      0.016  \n",
      "2      0.013  \n",
      "0      0.030  \n",
      "3      0.012  \n",
      "\n",
      "üèÜ Best baseline model: Random Forest\n",
      "   F1 Score: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - Baseline Results Analysis\n",
    "print(\"\\nüìä Baseline Model Results Summary\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create results DataFrame for easy comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(baseline_results.keys()),\n",
    "    'F1_Score': [results['f1_score'] for results in baseline_results.values()],\n",
    "    'ROC_AUC': [results['roc_auc'] for results in baseline_results.values()],\n",
    "    'Avg_Precision': [results['avg_precision'] for results in baseline_results.values()],\n",
    "    'CV_F1_Mean': [results['cv_f1_mean'] for results in baseline_results.values()],\n",
    "    'CV_F1_Std': [results['cv_f1_std'] for results in baseline_results.values()]\n",
    "})\n",
    "\n",
    "# Sort by F1 score\n",
    "results_df = results_df.sort_values('F1_Score', ascending=False)\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1_Score']\n",
    "\n",
    "print(f\"\\nüèÜ Best baseline model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {best_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "347bbf8c-d4e4-4383-8baa-c970b19037f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öñÔ∏è Advanced Sampling for Class Imbalance\n",
      "=============================================\n",
      "Testing sampling strategies with Random Forest (best tree-based model)...\n",
      "\n",
      "üîÑ Testing SMOTE...\n",
      "   Original: 1,383 samples\n",
      "   Resampled: 2,110 samples\n",
      "   New ratio: 1.0:1\n",
      "   F1 Score: 0.970\n",
      "   ROC-AUC: 0.999\n",
      "\n",
      "üîÑ Testing ADASYN...\n",
      "   Original: 1,383 samples\n",
      "   Resampled: 2,126 samples\n",
      "   New ratio: 1.0:1\n",
      "   F1 Score: 0.969\n",
      "   ROC-AUC: 0.998\n",
      "\n",
      "üîÑ Testing SMOTEENN...\n",
      "   Original: 1,383 samples\n",
      "   Resampled: 1,078 samples\n",
      "   New ratio: 0.9:1\n",
      "   F1 Score: 0.963\n",
      "   ROC-AUC: 0.998\n",
      "\n",
      "üîÑ Testing Random_Undersample...\n",
      "   Original: 1,383 samples\n",
      "   Resampled: 656 samples\n",
      "   New ratio: 1.0:1\n",
      "   F1 Score: 0.910\n",
      "   ROC-AUC: 0.995\n",
      "\n",
      "üìà Sampling Strategy Comparison:\n",
      "SMOTE           | F1: 0.970 | ROC-AUC: 0.999\n",
      "ADASYN          | F1: 0.969 | ROC-AUC: 0.998\n",
      "SMOTEENN        | F1: 0.963 | ROC-AUC: 0.998\n",
      "Random_Undersample | F1: 0.910 | ROC-AUC: 0.995\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 - Advanced Sampling Techniques\n",
    "print(\"\\n‚öñÔ∏è Advanced Sampling for Class Imbalance\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Test different sampling strategies\n",
    "sampling_strategies = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'SMOTEENN': SMOTEENN(random_state=42),\n",
    "    'Random_Undersample': RandomUnderSampler(random_state=42)\n",
    "}\n",
    "\n",
    "sampling_results = {}\n",
    "\n",
    "print(\"Testing sampling strategies with Random Forest (best tree-based model)...\")\n",
    "\n",
    "for strategy_name, sampler in sampling_strategies.items():\n",
    "    print(f\"\\nüîÑ Testing {strategy_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Apply sampling\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"   Original: {len(X_train):,} samples\")\n",
    "        print(f\"   Resampled: {len(X_resampled):,} samples\")\n",
    "        \n",
    "        # Check new class distribution\n",
    "        resampled_dist = pd.Series(y_resampled).value_counts()\n",
    "        print(f\"   New ratio: {resampled_dist[0]/resampled_dist[1]:.1f}:1\")\n",
    "        \n",
    "        # Train Random Forest on resampled data\n",
    "        rf_sampled = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_sampled.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Evaluate on original test set\n",
    "        y_pred_sampled = rf_sampled.predict(X_test)\n",
    "        y_pred_proba_sampled = rf_sampled.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        f1_sampled = f1_score(y_test, y_pred_sampled)\n",
    "        roc_auc_sampled = roc_auc_score(y_test, y_pred_proba_sampled)\n",
    "        \n",
    "        sampling_results[strategy_name] = {\n",
    "            'f1_score': f1_sampled,\n",
    "            'roc_auc': roc_auc_sampled,\n",
    "            'model': rf_sampled,\n",
    "            'y_pred': y_pred_sampled,\n",
    "            'y_pred_proba': y_pred_proba_sampled\n",
    "        }\n",
    "        \n",
    "        print(f\"   F1 Score: {f1_sampled:.3f}\")\n",
    "        print(f\"   ROC-AUC: {roc_auc_sampled:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "\n",
    "# Compare sampling strategies\n",
    "if sampling_results:\n",
    "    print(f\"\\nüìà Sampling Strategy Comparison:\")\n",
    "    for strategy, results in sampling_results.items():\n",
    "        print(f\"{strategy:15} | F1: {results['f1_score']:.3f} | ROC-AUC: {results['roc_auc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1495b0-9422-4f3e-a833-e43b3efb23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéõÔ∏è Hyperparameter Tuning\n",
      "==============================\n",
      "\n",
      "üîß Tuning Random Forest...\n",
      "   Best CV F1: 0.967\n",
      "   Test F1: 0.964\n",
      "   Test ROC-AUC: 0.998\n",
      "   Best params: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "üîß Tuning Gradient Boosting...\n",
      "   Best CV F1: 0.974\n",
      "   Test F1: 0.969\n",
      "   Test ROC-AUC: 0.992\n",
      "   Best params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Hyperparameter Tuning for Best Models\n",
    "print(\"\\nüéõÔ∏è Hyperparameter Tuning\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Focus on top 2 performing baseline models for tuning\n",
    "top_models = results_df.head(2)['Model'].tolist()\n",
    "\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    print(f\"\\nüîß Tuning {model_name}...\")\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']\n",
    "        }\n",
    "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        X_model = X_train\n",
    "        \n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        base_model = GradientBoostingClassifier(random_state=42)\n",
    "        X_model = X_train\n",
    "        \n",
    "    elif model_name == 'Logistic Regression':\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "        base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        X_model = X_train_scaled\n",
    "        \n",
    "    elif model_name == 'SVM':\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }\n",
    "        base_model = SVC(random_state=42, probability=True)\n",
    "        X_model = X_train_scaled\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_model, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if model_name in ['Logistic Regression', 'SVM']:\n",
    "        y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "        y_pred_proba_tuned = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_pred_tuned = best_model.predict(X_test)\n",
    "        y_pred_proba_tuned = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "    roc_auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "    \n",
    "    tuning_results[model_name] = {\n",
    "        'best_model': best_model,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'f1_score': f1_tuned,\n",
    "        'roc_auc': roc_auc_tuned,\n",
    "        'y_pred': y_pred_tuned,\n",
    "        'y_pred_proba': y_pred_proba_tuned\n",
    "    }\n",
    "    \n",
    "    print(f\"   Best CV F1: {grid_search.best_score_:.3f}\")\n",
    "    print(f\"   Test F1: {f1_tuned:.3f}\")\n",
    "    print(f\"   Test ROC-AUC: {roc_auc_tuned:.3f}\")\n",
    "    print(f\"   Best params: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963bc87d-0741-48da-98ac-aa26f471732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé≠ Ensemble Model Creation\n",
      "==============================\n",
      "Created ensemble with 2 models:\n",
      "  - Random Forest\n",
      "  - Gradient Boosting\n",
      "\n",
      "üî• Training ensemble model...\n",
      "   Ensemble F1: 0.969\n",
      "   Ensemble ROC-AUC: 0.998\n",
      "\n",
      "üéØ Model Training Complete!\n",
      "Ready for evaluation and interpretability analysis...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Ensemble Model Creation\n",
    "print(\"\\nüé≠ Ensemble Model Creation\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create ensemble of best performing models\n",
    "ensemble_models = []\n",
    "ensemble_names = []\n",
    "\n",
    "# Add tuned models to ensemble\n",
    "for name, results in tuning_results.items():\n",
    "    ensemble_models.append((name.lower().replace(' ', '_'), results['best_model']))\n",
    "    ensemble_names.append(name)\n",
    "\n",
    "# Create voting classifier\n",
    "if len(ensemble_models) >= 2:\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=ensemble_models,\n",
    "        voting='soft'  # Use probability averages\n",
    "    )\n",
    "    \n",
    "    print(f\"Created ensemble with {len(ensemble_models)} models:\")\n",
    "    for name in ensemble_names:\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    # Train ensemble\n",
    "    print(f\"\\nüî• Training ensemble model...\")\n",
    "    \n",
    "    # Need to determine which features to use (scaled vs unscaled)\n",
    "    # Use original features for tree-based, scaled for others\n",
    "    voting_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    y_pred_ensemble = voting_classifier.predict(X_test)\n",
    "    y_pred_proba_ensemble = voting_classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    f1_ensemble = f1_score(y_test, y_pred_ensemble)\n",
    "    roc_auc_ensemble = roc_auc_score(y_test, y_pred_proba_ensemble)\n",
    "    \n",
    "    print(f\"   Ensemble F1: {f1_ensemble:.3f}\")\n",
    "    print(f\"   Ensemble ROC-AUC: {roc_auc_ensemble:.3f}\")\n",
    "    \n",
    "    # Add ensemble to results\n",
    "    tuning_results['Ensemble'] = {\n",
    "        'best_model': voting_classifier,\n",
    "        'f1_score': f1_ensemble,\n",
    "        'roc_auc': roc_auc_ensemble,\n",
    "        'y_pred': y_pred_ensemble,\n",
    "        'y_pred_proba': y_pred_proba_ensemble\n",
    "    }\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough models for ensemble (need at least 2)\")\n",
    "\n",
    "print(f\"\\nüéØ Model Training Complete!\")\n",
    "print(f\"Ready for evaluation and interpretability analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72962a92-a572-4d13-84c8-f1a33333cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "üèÜ COMPREHENSIVE MODEL RANKING:\n",
      "                                 F1  ROC_AUC      Type\n",
      "Baseline_Random Forest        0.975    0.998  Baseline\n",
      "Sampled_SMOTE                 0.970    0.999   Sampled\n",
      "Tuned_Gradient Boosting       0.969    0.992     Tuned\n",
      "Baseline_Gradient Boosting    0.969    0.998  Baseline\n",
      "Tuned_Ensemble                0.969    0.998     Tuned\n",
      "Tuned_Random Forest           0.964    0.998     Tuned\n",
      "Baseline_Logistic Regression  0.777    0.950  Baseline\n",
      "Baseline_SVM                  0.422    0.770  Baseline\n",
      "\n",
      "ü•á CHAMPION MODEL: Baseline_Random Forest\n",
      "   F1 Score: 0.975\n",
      "   ROC-AUC:  0.998\n",
      "\n",
      "‚úÖ SUCCESS CRITERIA CHECK:\n",
      "   F1 Score > 0.65: ‚úÖ PASS\n",
      "   ROC-AUC > 0.85: ‚úÖ PASS\n",
      "   Multiple Models Trained: ‚úÖ PASS\n",
      "\n",
      "üöÄ PHASE 5 COMPLETE!\n",
      "==============================\n",
      "Next steps:\n",
      "1. Proceed to 06_model_evaluation.ipynb for detailed analysis\n",
      "2. SHAP interpretability analysis\n",
      "3. Feature importance and physics validation\n",
      "4. Error analysis and model insights\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - Model Evaluation Summary\n",
    "print(\"\\nüìä FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compile all results for comparison\n",
    "all_results = {}\n",
    "\n",
    "# Add baseline results\n",
    "for name, results in baseline_results.items():\n",
    "    all_results[f\"Baseline_{name}\"] = {\n",
    "        'F1': results['f1_score'],\n",
    "        'ROC_AUC': results['roc_auc'],\n",
    "        'Type': 'Baseline'\n",
    "    }\n",
    "\n",
    "# Add tuned results\n",
    "for name, results in tuning_results.items():\n",
    "    all_results[f\"Tuned_{name}\"] = {\n",
    "        'F1': results['f1_score'],\n",
    "        'ROC_AUC': results['roc_auc'],\n",
    "        'Type': 'Tuned'\n",
    "    }\n",
    "\n",
    "# Add best sampling result if available\n",
    "if sampling_results:\n",
    "    best_sampling = max(sampling_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "    all_results[f\"Sampled_{best_sampling[0]}\"] = {\n",
    "        'F1': best_sampling[1]['f1_score'],\n",
    "        'ROC_AUC': best_sampling[1]['roc_auc'],\n",
    "        'Type': 'Sampled'\n",
    "    }\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "final_results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
    "final_results_df = final_results_df.sort_values('F1', ascending=False)\n",
    "\n",
    "print(\"üèÜ COMPREHENSIVE MODEL RANKING:\")\n",
    "print(final_results_df.round(3))\n",
    "\n",
    "# Identify overall best model\n",
    "best_overall = final_results_df.index[0]\n",
    "best_f1_final = final_results_df.iloc[0]['F1']\n",
    "best_roc_final = final_results_df.iloc[0]['ROC_AUC']\n",
    "\n",
    "print(f\"\\nü•á CHAMPION MODEL: {best_overall}\")\n",
    "print(f\"   F1 Score: {best_f1_final:.3f}\")\n",
    "print(f\"   ROC-AUC:  {best_roc_final:.3f}\")\n",
    "\n",
    "# Success criteria check\n",
    "print(f\"\\n‚úÖ SUCCESS CRITERIA CHECK:\")\n",
    "success_criteria = {\n",
    "    'F1 Score > 0.65': best_f1_final > 0.65,\n",
    "    'ROC-AUC > 0.85': best_roc_final > 0.85,\n",
    "    'Multiple Models Trained': len(all_results) >= 5\n",
    "}\n",
    "\n",
    "for criterion, passed in success_criteria.items():\n",
    "    status = \"‚úÖ PASS\" if passed else \"‚ùå NEEDS IMPROVEMENT\"\n",
    "    print(f\"   {criterion}: {status}\")\n",
    "\n",
    "print(f\"\\nüöÄ PHASE 5 COMPLETE!\")\n",
    "print(\"=\"*30)\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Proceed to 06_model_evaluation.ipynb for detailed analysis\")\n",
    "print(\"2. SHAP interpretability analysis\")\n",
    "print(\"3. Feature importance and physics validation\")\n",
    "print(\"4. Error analysis and model insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc2637-8393-41c0-88c2-f219c4e0e304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
